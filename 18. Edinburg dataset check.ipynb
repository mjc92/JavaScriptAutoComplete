{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import astunparse\n",
    "import ast\n",
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(string):\n",
    "    return string.replace('DCSP',' ').replace(' DCNL','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for code-only-corpus\n",
    "bodies_dir = '/home/irteam/users/mjchoi/github/code-docstring-corpus/code-only-corpus/data_mono_ps.bodies'\n",
    "decl_dir = '/home/irteam/users/mjchoi/github/code-docstring-corpus/code-only-corpus/data_mono_ps.declarations'\n",
    "decl_body_dir = '/home/irteam/users/mjchoi/github/code-docstring-corpus/code-only-corpus/data_mono_ps.declbodies'\n",
    "meta_dir = '/home/irteam/users/mjchoi/github/code-docstring-corpus/code-only-corpus/data_mono_ps.metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(bodies_dir) as f:\n",
    "    bodies = f.read().split('\\n')\n",
    "with open(decl_dir) as f:\n",
    "    decl = f.read().split('\\n')\n",
    "with open(decl_body_dir) as f:\n",
    "    decl_body = f.read().split('\\n')\n",
    "with open(meta_dir) as f:\n",
    "    meta = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body:\n",
      "  with open(key_path, 'r') as f:\n",
      "   params = pem.parse_private_key(f.read())\n",
      "   privkey = rsakey.RSAKey(*params)\n",
      " with open(cert_path, 'r') as f:\n",
      "   s = f.read()\n",
      "   bList = pem.dePemList(s, 'CERTIFICATE')\n",
      " certificates = pb2.X509Certificates()\n",
      " certificates.certificate.extend(map(str, bList))\n",
      " pr.pki_type = 'x509+sha256'\n",
      " pr.pki_data = certificates.SerializeToString()\n",
      " msgBytes = bytearray(pr.SerializeToString())\n",
      " hashBytes = bytearray(hashlib.sha256(msgBytes).digest())\n",
      " sig = privkey.sign((x509.PREFIX_RSA_SHA256 + hashBytes))\n",
      " pr.signature = bytes(sig)\n",
      "=======================================================================\n",
      "Declaration:\n",
      " def sign_request_with_x509(pr, key_path, cert_path):\n",
      "=======================================================================\n",
      "Declaration & body:\n",
      "def sign_request_with_x509(pr, key_path, cert_path):\n",
      "  with open(key_path, 'r') as f:\n",
      "   params = pem.parse_private_key(f.read())\n",
      "   privkey = rsakey.RSAKey(*params)\n",
      " with open(cert_path, 'r') as f:\n",
      "   s = f.read()\n",
      "   bList = pem.dePemList(s, 'CERTIFICATE')\n",
      " certificates = pb2.X509Certificates()\n",
      " certificates.certificate.extend(map(str, bList))\n",
      " pr.pki_type = 'x509+sha256'\n",
      " pr.pki_data = certificates.SerializeToString()\n",
      " msgBytes = bytearray(pr.SerializeToString())\n",
      " hashBytes = bytearray(hashlib.sha256(msgBytes).digest())\n",
      " sig = privkey.sign((x509.PREFIX_RSA_SHA256 + hashBytes))\n",
      " pr.signature = bytes(sig)\n",
      "=======================================================================\n",
      "Metadata:\n",
      " github/spesmilo/electrum/lib/paymentrequest.py 410\n",
      "=======================================================================\n"
     ]
    }
   ],
   "source": [
    "idx = 1000\n",
    "print(\"Body:\")\n",
    "print(bodies[idx].replace('DCSP','').replace(' DCNL ','\\n'))\n",
    "print('=======================================================================')\n",
    "print(\"Declaration:\\n\",decl[idx])\n",
    "print('=======================================================================')\n",
    "print(\"Declaration & body:\")\n",
    "print(decl_body[idx].replace('DCSP','').replace(' DCNL ','\\n'))\n",
    "print('=======================================================================')\n",
    "print(\"Metadata:\\n\",meta[idx])\n",
    "print('=======================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bodies_test_dir = '/home/irteam/users/mjchoi/github/code-docstring-corpus/parallel-corpus/data_ps.bodies.test'\n",
    "decldesc_test_dir = '/home/irteam/users/mjchoi/github/code-docstring-corpus/parallel-corpus/data_ps.decldesc.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(bodies_test_dir) as f:\n",
    "    bodies1 = f.read().split('\\n')\n",
    "with open(decldesc_test_dir) as f:\n",
    "    bodies2 = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ body ----------------------------------\n",
      "   if cint(frappe.db.get_single_value(u'System   Settings', u'setup_complete')):\n",
      "      return\n",
      "   args = process_args(args)\n",
      "   try:\n",
      "      if (args.language and (args.language != u'english')):\n",
      "         set_default_language(get_language_code(args.lang))\n",
      "      frappe.clear_cache()\n",
      "      update_system_settings(args)\n",
      "      update_user_name(args)\n",
      "      for method in frappe.get_hooks(u'setup_wizard_complete'):\n",
      "         frappe.get_attr(method)(args)\n",
      "      disable_future_access()\n",
      "      frappe.db.commit()\n",
      "      frappe.clear_cache()\n",
      "   except:\n",
      "      frappe.db.rollback()\n",
      "      if args:\n",
      "         traceback = frappe.get_traceback()\n",
      "         for hook in frappe.get_hooks(u'setup_wizard_exception'):\n",
      "            frappe.get_attr(hook)(traceback, args)\n",
      "      raise\n",
      "   else:\n",
      "      for hook in frappe.get_hooks(u'setup_wizard_success'):\n",
      "         frappe.get_attr(hook)(args)\n",
      "------------------------ declaration ----------------------------------\n",
      "@frappe.whitelist()\n",
      " def setup_complete(args):\n",
      " 'Calls hooks for `setup_wizard_complete`, sets home page as `desktop`\n",
      " and clears cache. If wizard breaks, calls `setup_wizard_exception` hook'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   module = AnsibleModule(argument_spec=ClcGroup._define_module_argument_spec(), supports_check_mode=True)\n",
      "   clc_group = ClcGroup(module)\n",
      "   clc_group.process_request()\n",
      "------------------------ declaration ----------------------------------\n",
      "def main():\n",
      " 'The main function.  Instantiates the module and calls process_request.\n",
      " :return: none'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   try:\n",
      "      fields = os.path.split(filename)\n",
      "      if fields:\n",
      "         if (fields[(-1)].startswith('dataset_') and fields[(-1)].endswith('.dat')):\n",
      "            return Dataset.get(int(fields[(-1)][len('dataset_'):(- len('.dat'))]))\n",
      "   except:\n",
      "      pass\n",
      "   return None\n",
      "------------------------ declaration ----------------------------------\n",
      "def __guess_dataset_by_filename(filename):\n",
      " 'Return a guessed dataset by filename'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   result = []\n",
      "   for node in maybe_xrefs:\n",
      "      if isinstance(node, addnodes.pending_xref):\n",
      "         result.extend(resolve_pending_xref(app, fromdocname, node.deepcopy()))\n",
      "      else:\n",
      "         result.append(node)\n",
      "   return result\n",
      "------------------------ declaration ----------------------------------\n",
      "def resolve_possible_pending_xrefs(app, fromdocname, maybe_xrefs):\n",
      " 'If any node is a pending_xref, attempt to resolve it. If it cannot be\n",
      " resolved, replace it with its children.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   (is_travis, non_pr) = check_environment()\n",
      "   if (not is_travis):\n",
      "      return\n",
      "   if (not non_pr):\n",
      "      print('Running   in   Travis   during   non-merge   to   master,   doing   nothing.')\n",
      "      sys.exit(0)\n",
      "   decrypt_keyfile()\n",
      "------------------------ declaration ----------------------------------\n",
      "def prepare_to_run():\n",
      " 'Prepare to run system tests.\n",
      " If on Travis during a PR, exit the entire program; there is\n",
      " no need to run the system tests.\n",
      " If on Travis during a build for a non-PR merge to master,\n",
      " decrypts stored keyfile.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   result = script.pip('download', expect_error=True)\n",
      "   assert ('You   must   give   at   least   one   requirement   to   download' in result.stderr)\n",
      "   assert (result.returncode == ERROR)\n",
      "------------------------ declaration ----------------------------------\n",
      "def test_download_exit_status_code_when_no_requirements(script):\n",
      " 'Test download exit status code when no requirements specified'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   with open('../show_version.txt') as show_ver_file:\n",
      "      show_ver = show_ver_file.read()\n",
      "   print obtain_os_version(show_ver)\n",
      "------------------------ declaration ----------------------------------\n",
      "def main():\n",
      " 'Obtain the OS version from the show version output\n",
      " Print output to STDOUT'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   stdout.write(msg.encode('utf-8'))\n",
      "------------------------ declaration ----------------------------------\n",
      "def _out(msg):\n",
      " 'Output a string'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   api_key = bot.config.get('api_keys', {}).get('bing_azure')\n",
      "   show_nsfw = text.endswith('   nsfw')\n",
      "   if show_nsfw:\n",
      "      text = text[:(-5)].strip().lower()\n",
      "   rating = (NSFW_FILTER if show_nsfw else DEFAULT_FILTER)\n",
      "   if (not api_key):\n",
      "      return 'Error:   No   Bing   Azure   API   details.'\n",
      "   params = {'Sources': bingify('image'), 'Query': bingify(text), 'Adult': bingify(rating), '$format': 'json'}\n",
      "   request = requests.get(API_URL, params=params, auth=(api_key, api_key))\n",
      "   j = request.json()['d']['results'][0]\n",
      "   if (not j['Image']):\n",
      "      return 'No   results.'\n",
      "   result = random.choice(j['Image'][:10])\n",
      "   tags = []\n",
      "   tags.append('{}x{}px'.format(result['Width'], result['Height']))\n",
      "   tags.append(result['ContentType'])\n",
      "   tags.append(filesize.size(int(result['FileSize']), system=filesize.alternative))\n",
      "   if ('explicit' in result['Thumbnail']['MediaUrl']):\n",
      "      tags.append('NSFW')\n",
      "   tag_text = ',   '.join(tags)\n",
      "   return '{}   ({})'.format(unescape(result['MediaUrl']), tag_text)\n",
      "------------------------ declaration ----------------------------------\n",
      "@hook.command('bingimage', 'bis')\n",
      " def bingimage(text, bot):\n",
      " '<query> - returns the first bing image search result for <query>'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (not issubclass(optionable_type, Optionable)):\n",
      "      raise TypeError(u'The   given   `optionable_type`   was   not   a   subclass   of   `Optionable`:   {}'.format(optionable_type))\n",
      "   option_values = {}\n",
      "   registration_function = _options_registration_function(option_values)\n",
      "   optionable_type.register_options(registration_function)\n",
      "   option_values.update(**options)\n",
      "   return create_option_values(option_values)\n",
      "------------------------ declaration ----------------------------------\n",
      "def create_option_values_for_optionable(optionable_type, **options):\n",
      " 'Create a fake OptionValueContainer with appropriate defaults for the given `Optionable` type.\"\n",
      " :param type optionable_type: An :class:`pants.option.optionable.Optionable` subclass.\n",
      " :param **options: Keyword args representing option values explicitly set via the command line.\n",
      " :returns: A fake `OptionValueContainer`, ie: the value returned from `get_options()`.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   q = ((2 * endog) - 1)\n",
      "   X = exog\n",
      "   return np.add.reduce(stats.norm.logcdf((q * np.dot(X, params))))\n",
      "------------------------ declaration ----------------------------------\n",
      "def probitloglike(params, endog, exog):\n",
      " 'Log likelihood for the probit'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   filename = get_pkg_data_filename(u'data/sip2.fits')\n",
      "   with catch_warnings(wcs.wcs.FITSFixedWarning) as caught_warnings:\n",
      "      ww = wcs.WCS(filename)\n",
      "      assert (len(caught_warnings) == 1)\n",
      "   n = 3\n",
      "   pixels = (np.arange(n) * np.ones((2, n))).T\n",
      "   result = ww.wcs_pix2world(pixels, 0, ra_dec_order=True)\n",
      "   ww.wcs_pix2world(pixels[..., 0], pixels[..., 1], 0, ra_dec_order=True)\n",
      "   close_enough = 1e-08\n",
      "   answer = np.array([[0.00024976, 0.00023018], [0.00023043, (-0.00024997)]])\n",
      "   assert np.all((np.abs((ww.wcs.pc - answer)) < close_enough))\n",
      "   answer = np.array([[202.39265216, 47.17756518], [202.39335826, 47.17754619], [202.39406436, 47.1775272]])\n",
      "   assert np.all((np.abs((result - answer)) < close_enough))\n",
      "------------------------ declaration ----------------------------------\n",
      "def test_pix2world():\n",
      " 'From github issue #1463'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   (fig, ax) = utils.create_mpl_ax(ax)\n",
      "   infl = results.get_influence()\n",
      "   if criterion.lower().startswith('coo'):\n",
      "      psize = infl.cooks_distance[0]\n",
      "   elif criterion.lower().startswith('dff'):\n",
      "      psize = np.abs(infl.dffits[0])\n",
      "   else:\n",
      "      raise ValueError(('Criterion   %s   not   understood' % criterion))\n",
      "   old_range = np.ptp(psize)\n",
      "   new_range = ((size ** 2) - (8 ** 2))\n",
      "   psize = ((((psize - psize.min()) * new_range) / old_range) + (8 ** 2))\n",
      "   leverage = infl.hat_matrix_diag\n",
      "   if external:\n",
      "      resids = infl.resid_studentized_external\n",
      "   else:\n",
      "      resids = infl.resid_studentized_internal\n",
      "   from scipy import stats\n",
      "   cutoff = stats.t.ppf((1.0 - (alpha / 2)), results.df_resid)\n",
      "   large_resid = (np.abs(resids) > cutoff)\n",
      "   large_leverage = (leverage > _high_leverage(results))\n",
      "   large_points = np.logical_or(large_resid, large_leverage)\n",
      "   ax.scatter(leverage, resids, s=psize, alpha=plot_alpha)\n",
      "   labels = results.model.data.row_labels\n",
      "   if (labels is None):\n",
      "      labels = lrange(len(resids))\n",
      "   ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resids), lzip((- ((psize / 2) ** 0.5)), ((psize / 2) ** 0.5)), 'x-large', ax)\n",
      "   font = {'fontsize': 16, 'color': 'black'}\n",
      "   ax.set_ylabel('Studentized   Residuals', **font)\n",
      "   ax.set_xlabel('H   Leverage', **font)\n",
      "   ax.set_title('Influence   Plot', **font)\n",
      "   return fig\n",
      "------------------------ declaration ----------------------------------\n",
      "def influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n",
      " 'Plot of influence in regression. Plots studentized resids vs. leverage.\n",
      " Parameters\n",
      " results : results instance\n",
      " A fitted model.\n",
      " external : bool\n",
      " Whether to use externally or internally studentized residuals. It is\n",
      " recommended to leave external as True.\n",
      " alpha : float\n",
      " The alpha value to identify large studentized residuals. Large means\n",
      " abs(resid_studentized) > t.ppf(1-alpha/2, dof=results.df_resid)\n",
      " criterion : str {\\'DFFITS\\', \\'Cooks\\'}\n",
      " Which criterion to base the size of the points on. Options are\n",
      " DFFITS or Cook\\'s D.\n",
      " size : float\n",
      " The range of `criterion` is mapped to 10**2 - size**2 in points.\n",
      " plot_alpha : float\n",
      " The `alpha` of the plotted points.\n",
      " ax : matplotlib Axes instance\n",
      " An instance of a matplotlib Axes.\n",
      " Returns\n",
      " fig : matplotlib figure\n",
      " The matplotlib figure that contains the Axes.\n",
      " Notes\n",
      " Row labels for the observations in which the leverage, measured by the\n",
      " diagonal of the hat matrix, is high or the residuals are large, as the\n",
      " combination of large residuals and a high influence value indicates an\n",
      " influence point. The value of large residuals can be controlled using the\n",
      " `alpha` parameter. Large leverage points are identified as\n",
      " hat_i > 2 * (df_model + 1)/nobs.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   nf = NetlogFile(name)\n",
      "   nf.sock.sendall(data)\n",
      "   nf.close()\n",
      "------------------------ declaration ----------------------------------\n",
      "def send_file(name, data):\n",
      " 'Send file to result server'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   root = get_or_create_root()\n",
      "   return redirect('wiki:get', path=root.path)\n",
      "------------------------ declaration ----------------------------------\n",
      "def root_create(request):\n",
      " 'In the edX wiki, we don\\'t show the root_create view. Instead, we\n",
      " just create the root automatically if it doesn\\'t exist.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return IMPL.block_device_mapping_update_or_create(context, values)\n",
      "------------------------ declaration ----------------------------------\n",
      "def block_device_mapping_update_or_create(context, values):\n",
      " 'Update an entry of block device mapping.\n",
      " If not existed, create a new entry'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   flavors = novaclient(request).flavors.list(is_public=is_public)\n",
      "   if get_extras:\n",
      "      for flavor in flavors:\n",
      "         flavor.extras = flavor_get_extras(request, flavor.id, True, flavor)\n",
      "   return flavors\n",
      "------------------------ declaration ----------------------------------\n",
      "@profiler.trace\n",
      " @memoized\n",
      " def flavor_list(request, is_public=True, get_extras=False):\n",
      " 'Get the list of available instance sizes (flavors).'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (u'charset' in format):\n",
      "      return format\n",
      "   if (format in (u'application/json', u'text/javascript')):\n",
      "      return format\n",
      "   return (u'%s;   charset=%s' % (format, encoding))\n",
      "------------------------ declaration ----------------------------------\n",
      "def build_content_type(format, encoding=u'utf-8'):\n",
      " 'Appends character encoding to the provided format if not already present.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   random_index = randint(0, (TopicCategory.objects.all().count() - 1))\n",
      "   tc = TopicCategory.objects.all()[random_index]\n",
      "   resource.category = tc\n",
      "   resource.save()\n",
      "------------------------ declaration ----------------------------------\n",
      "def assign_random_category(resource):\n",
      " 'Assign a random category to a resource'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   NOT_PRESENT = object()\n",
      "   old_values = {}\n",
      "   for (attr, new_value) in kwargs.items():\n",
      "      old_values[attr] = getattr(obj, attr, NOT_PRESENT)\n",
      "      setattr(obj, attr, new_value)\n",
      "   try:\n",
      "      (yield)\n",
      "   finally:\n",
      "      for (attr, old_value) in old_values.items():\n",
      "         if (old_value is NOT_PRESENT):\n",
      "            del obj[attr]\n",
      "         else:\n",
      "            setattr(obj, attr, old_value)\n",
      "------------------------ declaration ----------------------------------\n",
      "@contextlib.contextmanager\n",
      " def temporary_mutation(obj, **kwargs):\n",
      " 'Temporarily set the attr on a particular object to a given value then\n",
      " revert when finished.\n",
      " One use of this is to temporarily set the read_deleted flag on a context\n",
      " object:\n",
      " with temporary_mutation(context, read_deleted=\"yes\"):\n",
      " do_something_that_needed_deleted_objects()'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   modules = set(['textannotation', 'imageannotation', 'videoannotation'])\n",
      "   return bool(modules.intersection(course.advanced_modules))\n",
      "------------------------ declaration ----------------------------------\n",
      "def is_harvard_notes_enabled(course):\n",
      " 'Returns True if Harvard Annotation Tool is enabled for the course,\n",
      " False otherwise.\n",
      " Checks for \\'textannotation\\', \\'imageannotation\\', \\'videoannotation\\' in the list\n",
      " of advanced modules of the course.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   for (label, seq) in parse_fasta(file_lines):\n",
      "      if (id_from_fasta_label_line(label) in ids):\n",
      "         (yield (label, seq))\n",
      "------------------------ declaration ----------------------------------\n",
      "def seqs_from_file(ids, file_lines):\n",
      " 'Extract labels and seqs from file'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   inse = tf.reduce_sum((output * target))\n",
      "   l = tf.reduce_sum((output * output))\n",
      "   r = tf.reduce_sum((target * target))\n",
      "   dice = ((2 * inse) / (l + r))\n",
      "   if (epsilon == 0):\n",
      "      return dice\n",
      "   else:\n",
      "      return tf.clip_by_value(dice, 0, (1.0 - epsilon))\n",
      "------------------------ declaration ----------------------------------\n",
      "def dice_coe(output, target, epsilon=1e-10):\n",
      " 'Sørensen–Dice coefficient for comparing the similarity of two distributions,\n",
      " usually be used for binary image segmentation i.e. labels are binary.\n",
      " The coefficient = [0, 1], 1 if totally match.\n",
      " Parameters\n",
      " output : tensor\n",
      " A distribution with shape: [batch_size, ....], (any dimensions).\n",
      " target : tensor\n",
      " A distribution with shape: [batch_size, ....], (any dimensions).\n",
      " epsilon : float\n",
      " An optional name to attach to this layer.\n",
      " Examples\n",
      " >>> outputs = tl.act.pixel_wise_softmax(network.outputs)\n",
      " >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_, epsilon=1e-5)\n",
      " References\n",
      " - `wiki-dice <https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient>`_'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if callable(getattr(self.connection.features, 'confirm', None)):\n",
      "      self.connection.features.confirm()\n",
      "   elif hasattr(self, '_rollback_works'):\n",
      "      can_rollback = self._rollback_works()\n",
      "      self.connection.settings_dict['SUPPORTS_TRANSACTIONS'] = can_rollback\n",
      "   return self._get_test_db_name()\n",
      "------------------------ declaration ----------------------------------\n",
      "def _skip_create_test_db(self, verbosity=1, autoclobber=False, serialize=True):\n",
      " '``create_test_db`` implementation that skips both creation and flushing\n",
      " The idea is to re-use the perfectly good test DB already created by an\n",
      " earlier test run, cutting the time spent before any tests run from 5-13s\n",
      " (depending on your I/O luck) down to 3.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   missionlist = readmission(aFileName)\n",
      "   print ('\\nUpload   mission   from   a   file:   %s' % import_mission_filename)\n",
      "   print '   Clear   mission'\n",
      "   cmds = vehicle.commands\n",
      "   cmds.clear()\n",
      "   for command in missionlist:\n",
      "      cmds.add(command)\n",
      "   print '   Upload   mission'\n",
      "   vehicle.commands.upload()\n",
      "------------------------ declaration ----------------------------------\n",
      "def upload_mission(aFileName):\n",
      " 'Upload a mission from a file.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (math_expr.strip() == ''):\n",
      "      return float('nan')\n",
      "   math_interpreter = ParseAugmenter(math_expr, case_sensitive)\n",
      "   math_interpreter.parse_algebra()\n",
      "   (all_variables, all_functions) = add_defaults(variables, functions, case_sensitive)\n",
      "   math_interpreter.check_variables(all_variables, all_functions)\n",
      "   if case_sensitive:\n",
      "      casify = (lambda x: x)\n",
      "   else:\n",
      "      casify = (lambda x: x.lower())\n",
      "   evaluate_actions = {'number': eval_number, 'variable': (lambda x: all_variables[casify(x[0])]), 'function': (lambda x: all_functions[casify(x[0])](x[1])), 'atom': eval_atom, 'power': eval_power, 'parallel': eval_parallel, 'product': eval_product, 'sum': eval_sum}\n",
      "   return math_interpreter.reduce_tree(evaluate_actions)\n",
      "------------------------ declaration ----------------------------------\n",
      "def evaluator(variables, functions, math_expr, case_sensitive=False):\n",
      " 'Evaluate an expression; that is, take a string of math and return a float.\n",
      " -Variables are passed as a dictionary from string to value. They must be\n",
      " python numbers.\n",
      " -Unary functions are passed as a dictionary from string to function.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (not data):\n",
      "      raise ValueError('No   data   available   for   pie   chart.')\n",
      "   all_fracs = []\n",
      "   all_labels = []\n",
      "   colors = []\n",
      "   for (color_ix, (c_label, c_frac)) in enumerate(data):\n",
      "      if (c_label == others_key):\n",
      "         colors.append(others_color)\n",
      "      else:\n",
      "         colors.append(data_colors[pref_colors[c_label]].toHex())\n",
      "      all_fracs.append(c_frac)\n",
      "      if should_capitalize:\n",
      "         capital = ('%s   (%.2f%%)' % (c_label.capitalize(), (c_frac * 100.0)))\n",
      "         all_labels.append(capital)\n",
      "      else:\n",
      "         all_labels.append(('%s   (%.2f%%)' % (c_label, (c_frac * 100.0))))\n",
      "   rc('font', size='10')\n",
      "   rc('text', color=label_color)\n",
      "   rc('patch', linewidth=0.1)\n",
      "   rc('axes', linewidth=0.5, edgecolor=label_color)\n",
      "   rc('text', usetex=False)\n",
      "   fig = figure(randrange(10000), figsize=(plot_width, plot_height))\n",
      "   fp = FontProperties()\n",
      "   fp.set_size('8')\n",
      "   if (len(data) > 30):\n",
      "      loc = 4\n",
      "   else:\n",
      "      loc = 5\n",
      "   mtitle = 'Pie   Chart'\n",
      "   if ('title' in props):\n",
      "      mtitle = props['title']\n",
      "   axis('off')\n",
      "   title(mtitle, fontsize='10', color=label_color)\n",
      "   ax = axes([0.0, 0.0, 0.5, 1])\n",
      "   p1 = pie(all_fracs, shadow=False, colors=colors)\n",
      "   if (file_prefix is None):\n",
      "      img_name = make_img_name(file_ext='.png')\n",
      "   else:\n",
      "      img_name = file_prefix\n",
      "   img_abs = os.path.join(dir_path, 'charts', img_name)\n",
      "   savefig(img_abs, dpi=dpi, facecolor=background_color)\n",
      "   eps_link = ''\n",
      "   eps_abs = ''\n",
      "   if (file_prefix is None):\n",
      "      eps_img_name = make_img_name(file_ext=('.%s' % generate_image_type))\n",
      "   else:\n",
      "      eps_img_name = (file_prefix + ('.%s' % generate_image_type))\n",
      "   savefig(os.path.join(dir_path, 'charts', eps_img_name), facecolor=background_color)\n",
      "   if (generate_image_type == 'eps'):\n",
      "      strip_eps_font(os.path.join(dir_path, 'charts', eps_img_name))\n",
      "   eps_abs = os.path.join(dir_path, 'charts', eps_img_name)\n",
      "   eps_link = (PDF_LINK % (os.path.join('charts', eps_img_name), ('View   Figure   (.%s)' % generate_image_type)))\n",
      "   close(fig)\n",
      "   clf()\n",
      "   updated_taxa = []\n",
      "   updated_colors = []\n",
      "   for i in data:\n",
      "      if (i[0] != others_key):\n",
      "         updated_taxa.append(i[0].replace('\"', ''))\n",
      "         updated_colors.append(data_colors[pref_colors[i[0]]].toHex())\n",
      "      else:\n",
      "         updated_taxa.append(others_key)\n",
      "         updated_colors.append(others_color)\n",
      "   if include_html_legend:\n",
      "      legend_fname_png = make_legend(updated_taxa, updated_colors, plot_width, plot_height, label_color, background_color, img_abs, 'png', 80)\n",
      "      legend_fpath_png = os.path.join('charts', legend_fname_png)\n",
      "   legend_fname = make_legend(updated_taxa, updated_colors, plot_width, plot_height, label_color, background_color, img_abs, generate_image_type, dpi)\n",
      "   legend_fpath = os.path.join('charts', legend_fname)\n",
      "   legend_link = (LEGEND_LINK % (legend_fpath, ('View   Legend   (.%s)' % generate_image_type)))\n",
      "   points_id = ''\n",
      "   xmap_html = ''\n",
      "   if (not include_html_legend):\n",
      "      IMG_TEXT = (IMG_SRC_minus_legend % (os.path.join('charts', img_name), points_id))\n",
      "   else:\n",
      "      IMG_TEXT = (IMG_SRC_2 % (os.path.join('charts', img_name), points_id, legend_fpath_png))\n",
      "   return (eps_link, legend_link, IMG_TEXT, xmap_html)\n",
      "------------------------ declaration ----------------------------------\n",
      "def make_pie_chart(data, dir_path, level, prefs, pref_colors, background_color, label_color, generate_image_type, plot_width, plot_height, bar_width, dpi, include_html_legend, file_prefix=None, props={}, others_key='All   Other   Categories', others_color='#eeeeee', should_capitalize=True):\n",
      " 'Write interactive piechart\n",
      " data: [fraction:label,...]\n",
      " trunc_len: truncates labels after this many chars'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   try:\n",
      "      return ismount_raw(path)\n",
      "   except OSError:\n",
      "      return False\n",
      "------------------------ declaration ----------------------------------\n",
      "def ismount(path):\n",
      " 'Test whether a path is a mount point. This will catch any\n",
      " exceptions and translate them into a False return value\n",
      " Use ismount_raw to have the exceptions raised instead.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   f = (_Cfunctions.get('libvlc_vlm_get_media_instance_title', None) or _Cfunction('libvlc_vlm_get_media_instance_title', ((1,), (1,), (1,)), None, ctypes.c_int, Instance, ctypes.c_char_p, ctypes.c_int))\n",
      "   return f(p_instance, psz_name, i_instance)\n",
      "------------------------ declaration ----------------------------------\n",
      "def libvlc_vlm_get_media_instance_title(p_instance, psz_name, i_instance):\n",
      " 'Get vlm_media instance title number by name or instance id.\n",
      " @param p_instance: a libvlc instance.\n",
      " @param psz_name: name of vlm media instance.\n",
      " @param i_instance: instance id.\n",
      " @return: title as number or -1 on error.\n",
      " @bug: will always return 0.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (len(s) < 1):\n",
      "      return Raw(s)\n",
      "   ietype = ord(s[0])\n",
      "   cls = ietypecls.get(ietype, Raw)\n",
      "   if ((cls == Raw) and ((ietype & 128) == 128)):\n",
      "      cls = IE_NotImplementedTLV\n",
      "   return cls(s)\n",
      "------------------------ declaration ----------------------------------\n",
      "def IE_Dispatcher(s):\n",
      " 'Choose the correct Information Element class.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   results = []\n",
      "   for path in combine_lists(*path_seqs):\n",
      "      expanded = expand_path(path)\n",
      "      paths = (sorted(glob.glob(expanded)) or [expanded])\n",
      "      results.extend(paths)\n",
      "   return results\n",
      "------------------------ declaration ----------------------------------\n",
      "def combine_path_lists(*path_seqs):\n",
      " 'Concatenate the given sequences into a list. Ignore None values.\n",
      " Resolve ``~`` (home dir) and environment variables, and expand globs\n",
      " that refer to the local filesystem.\n",
      " .. versionchanged:: 0.4.6\n",
      " Can take single strings as well as lists.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   atoi = (lambda addr: struct.unpack('!I', socket.inet_aton(addr))[0])\n",
      "   itoa = (lambda addr: socket.inet_ntoa(struct.pack('!I', addr)))\n",
      "   (address, netmask) = network.split('/')\n",
      "   netmask_i = ((4294967295 << (32 - atoi(netmask))) & 4294967295)\n",
      "   return itoa(((atoi(address) & netmask_i) + 1))\n",
      "------------------------ declaration ----------------------------------\n",
      "def first_ip(network):\n",
      " 'Return the first IPv4 address in network\n",
      " Args:\n",
      " network (str): network in CIDR format\n",
      " Returns:\n",
      " str: first IPv4 address'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   assert (name in ['harry', 'terry', 'ule'])\n",
      "   common = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'sparse', (name + '_'))\n",
      "   (trname, vname, tename) = [((common + subset) + '.npy') for subset in ['train', 'valid', 'test']]\n",
      "   train = load_sparse(trname)\n",
      "   valid = load_sparse(vname)\n",
      "   test = load_sparse(tename)\n",
      "   if randomize_valid:\n",
      "      rng = make_np_rng(None, [1, 2, 3, 4], which_method='permutation')\n",
      "      perm = rng.permutation(valid.shape[0])\n",
      "      valid = valid[perm]\n",
      "   if randomize_test:\n",
      "      rng = make_np_rng(None, [1, 2, 3, 4], which_method='permutation')\n",
      "      perm = rng.permutation(test.shape[0])\n",
      "      test = test[perm]\n",
      "   if normalize:\n",
      "      if (name == 'ule'):\n",
      "         train = (train.astype(theano.config.floatX) / 255)\n",
      "         valid = (valid.astype(theano.config.floatX) / 255)\n",
      "         test = (test.astype(theano.config.floatX) / 255)\n",
      "      elif (name == 'harry'):\n",
      "         train = train.astype(theano.config.floatX)\n",
      "         valid = valid.astype(theano.config.floatX)\n",
      "         test = test.astype(theano.config.floatX)\n",
      "         std = 0.6933604603392579\n",
      "         train = (train / std)\n",
      "         valid = (valid / std)\n",
      "         test = (test / std)\n",
      "      elif (name == 'terry'):\n",
      "         train = train.astype(theano.config.floatX)\n",
      "         valid = valid.astype(theano.config.floatX)\n",
      "         test = test.astype(theano.config.floatX)\n",
      "         train = (train / 300)\n",
      "         valid = (valid / 300)\n",
      "         test = (test / 300)\n",
      "      else:\n",
      "         raise Exception(\"This   dataset   don't   have   its   normalization   defined\")\n",
      "   if transfer:\n",
      "      fname = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'filetensor', (name + '_transfer.ft'))\n",
      "      transfer = load_filetensor(fname)\n",
      "      return (train, valid, test, transfer)\n",
      "   else:\n",
      "      return (train, valid, test)\n",
      "------------------------ declaration ----------------------------------\n",
      "def load_sparse_dataset(name, normalize=True, transfer=False, randomize_valid=False, randomize_test=False):\n",
      " 'Load the train,valid,test data for the dataset `name` and return it in\n",
      " sparse format.\n",
      " We suppose the data was created with ift6266h11/pretraitement/to_npy.py\n",
      " that shuffle the train. So the train should already be shuffled.\n",
      " name : \\'avicenna\\', \\'harry\\', \\'rita\\', \\'sylvester\\' or \\'ule\\'\n",
      " Which dataset to load\n",
      " normalize : bool\n",
      " If True, we normalize the train dataset before returning it\n",
      " transfer :\n",
      " If True also return the transfer label\n",
      " randomize_valid : bool\n",
      " Do we randomize the order of the valid set?  We always use the same\n",
      " random order If False, return in the same order as downloaded on the\n",
      " web\n",
      " randomize_test : bool\n",
      " Do we randomize the order of the test set?  We always use the same\n",
      " random order If False, return in the same order as downloaded on the\n",
      " web\n",
      " Returns\n",
      " train, valid, test : ndarrays\n",
      " Datasets returned if transfer = False\n",
      " train, valid, test, transfer : ndarrays\n",
      " Datasets returned if transfer = False'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   f = (_Cfunctions.get('libvlc_video_get_spu_description', None) or _Cfunction('libvlc_video_get_spu_description', ((1,),), None, ctypes.POINTER(TrackDescription), MediaPlayer))\n",
      "   return f(p_mi)\n",
      "------------------------ declaration ----------------------------------\n",
      "def libvlc_video_get_spu_description(p_mi):\n",
      " 'Get the description of available video subtitles.\n",
      " @param p_mi: the media player.\n",
      " @return: list containing description of available video subtitles. It must be freed with L{libvlc_track_description_list_release}().'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   arr = np.asarray(data, dtype=np.float32)\n",
      "   out = cuda.device_array(1, dtype=np.float32)\n",
      "   gpu_single_block_sum[(1, gpu_block_sum_max_blockdim)](arr, out)\n",
      "   return out.copy_to_host()[0]\n",
      "------------------------ declaration ----------------------------------\n",
      "def sum_parts(data):\n",
      " 'Driver for ``gpu_single_block_sum`` kernel'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   assert hug.validate.contains_one_of('no', 'way')(TEST_SCHEMA)\n",
      "   assert (not hug.validate.contains_one_of('last', 'place')(TEST_SCHEMA))\n",
      "------------------------ declaration ----------------------------------\n",
      "def test_contains_one_of():\n",
      " 'Test to ensure hug\\'s contains_one_of validation function works as expected to ensure presence of a field'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (len(nestedRings) > 0):\n",
      "      oldOrderedLocation.z = nestedRings[0].z\n",
      "   closestDistance = 9.876543219876543e+17\n",
      "   closestNestedRing = None\n",
      "   for remainingNestedRing in nestedRings:\n",
      "      distance = getClosestDistanceIndexToLine(oldOrderedLocation.dropAxis(), remainingNestedRing.boundary).distance\n",
      "      if (distance < closestDistance):\n",
      "         closestDistance = distance\n",
      "         closestNestedRing = remainingNestedRing\n",
      "   nestedRings.remove(closestNestedRing)\n",
      "   closestNestedRing.addToThreads(extrusionHalfWidth, oldOrderedLocation, skein, threadSequence)\n",
      "   return closestNestedRing\n",
      "------------------------ declaration ----------------------------------\n",
      "def getTransferClosestNestedRing(extrusionHalfWidth, nestedRings, oldOrderedLocation, skein, threadSequence):\n",
      " 'Get and transfer the closest remaining nested ring.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   requires = IS_ONE_OF(current.db, 'org_organisation.id', org_OrganisationRepresent(), realms=realms, updateable=updateable, orderby='org_organisation.name', sort=True)\n",
      "   if (not required):\n",
      "      requires = IS_EMPTY_OR(requires)\n",
      "   return requires\n",
      "------------------------ declaration ----------------------------------\n",
      "def org_organisation_requires(required=False, realms=None, updateable=False):\n",
      " '@param required: Whether the selection is optional or mandatory\n",
      " @param realms: Whether the list should be filtered to just those\n",
      " belonging to a list of realm entities\n",
      " @param updateable: Whether the list should be filtered to just those\n",
      " which the user has Write access to\n",
      " @ToDo: Option to remove Branches\n",
      " @ToDo: Option to only include Branches'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return 'emoncms{}_{}_{}_{}_{}'.format(sensorid, feedtag, feedname, feedid, feeduserid)\n",
      "------------------------ declaration ----------------------------------\n",
      "def get_id(sensorid, feedtag, feedname, feedid, feeduserid):\n",
      " 'Return unique identifier for feed / sensor.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return __execute_cmd('setniccfg   -s   {0}   {1}   {2}'.format(ip, netmask, gateway))\n",
      "------------------------ declaration ----------------------------------\n",
      "def set_network(ip, netmask, gateway):\n",
      " 'Configure Network\n",
      " CLI Example:\n",
      " .. code-block:: bash\n",
      " salt dell drac.set_network [DRAC IP] [NETMASK] [GATEWAY]\n",
      " salt dell drac.set_network 192.168.0.2 255.255.255.0 192.168.0.1'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   with smart_open(fname, 'wb') as fout:\n",
      "      _pickle.dump(obj, fout, protocol=protocol)\n",
      "------------------------ declaration ----------------------------------\n",
      "def pickle(obj, fname, protocol=2):\n",
      " 'Pickle object `obj` to file `fname`.\n",
      " `protocol` defaults to 2 so pickled objects are compatible across\n",
      " Python 2.x and 3.x.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return (None, changed_file)\n",
      "------------------------ declaration ----------------------------------\n",
      "def split_file_dummy(changed_file):\n",
      " 'Split the repository-relative filename into a tuple of (branchname,\n",
      " branch_relative_filename). If you have no branches, this should just\n",
      " return (None, changed_file).'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   entries1 = _tree_entries(path, tree1)\n",
      "   entries2 = _tree_entries(path, tree2)\n",
      "   i1 = i2 = 0\n",
      "   len1 = len(entries1)\n",
      "   len2 = len(entries2)\n",
      "   result = []\n",
      "   while ((i1 < len1) and (i2 < len2)):\n",
      "      entry1 = entries1[i1]\n",
      "      entry2 = entries2[i2]\n",
      "      if (entry1.path < entry2.path):\n",
      "         result.append((entry1, _NULL_ENTRY))\n",
      "         i1 += 1\n",
      "      elif (entry1.path > entry2.path):\n",
      "         result.append((_NULL_ENTRY, entry2))\n",
      "         i2 += 1\n",
      "      else:\n",
      "         result.append((entry1, entry2))\n",
      "         i1 += 1\n",
      "         i2 += 1\n",
      "   for i in range(i1, len1):\n",
      "      result.append((entries1[i], _NULL_ENTRY))\n",
      "   for i in range(i2, len2):\n",
      "      result.append((_NULL_ENTRY, entries2[i]))\n",
      "   return result\n",
      "------------------------ declaration ----------------------------------\n",
      "def _merge_entries(path, tree1, tree2):\n",
      " 'Merge the entries of two trees.\n",
      " :param path: A path to prepend to all tree entry names.\n",
      " :param tree1: The first Tree object to iterate, or None.\n",
      " :param tree2: The second Tree object to iterate, or None.\n",
      " :return: A list of pairs of TreeEntry objects for each pair of entries in\n",
      " the trees. If an entry exists in one tree but not the other, the other\n",
      " entry will have all attributes set to None. If neither entry\\'s path is\n",
      " None, they are guaranteed to match.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   req_vars = form.request_vars\n",
      "   position = req_vars.get('position', '')\n",
      "   reason = req_vars.get('reason', '')\n",
      "   db = current.db\n",
      "   table = db.auth_user\n",
      "   db((table.id == form.vars.id)).update(comments=('%s   |   %s' % (position, reason)))\n",
      "------------------------ declaration ----------------------------------\n",
      "def register_onaccept(form):\n",
      " 'Tasks to be performed after a new user registers'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (not a):\n",
      "      return []\n",
      "   else:\n",
      "      return [((a * b) % p) for b in f]\n",
      "------------------------ declaration ----------------------------------\n",
      "def gf_mul_ground(f, a, p, K):\n",
      " 'Compute ``f * a`` where ``f`` in ``GF(p)[x]`` and ``a`` in ``GF(p)``.\n",
      " Examples\n",
      " >>> from sympy.polys.domains import ZZ\n",
      " >>> from sympy.polys.galoistools import gf_mul_ground\n",
      " >>> gf_mul_ground([3, 2, 4], 2, 5, ZZ)\n",
      " [1, 4, 3]'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   review_request_id_to_commits_map = defaultdict(list)\n",
      "   branch_name = payload.get(u'repository_path')\n",
      "   if (not branch_name):\n",
      "      return review_request_id_to_commits_map\n",
      "   revisions = payload.get(u'revisions', [])\n",
      "   for revision in revisions:\n",
      "      revision_id = revision.get(u'revision')\n",
      "      if (len(revision_id) > 7):\n",
      "         revision_id = revision_id[:7]\n",
      "      commit_message = revision.get(u'message')\n",
      "      review_request_id = get_review_request_id(commit_message, server_url)\n",
      "      review_request_id_to_commits_map[review_request_id].append((u'%s   (%s)' % (branch_name, revision_id)))\n",
      "   return review_request_id_to_commits_map\n",
      "------------------------ declaration ----------------------------------\n",
      "def close_review_requests(payload, server_url):\n",
      " 'Closes all review requests for the Google Code repository.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   try:\n",
      "      codecs.lookup(argument)\n",
      "   except LookupError:\n",
      "      raise ValueError(('unknown   encoding:   \"%s\"' % argument))\n",
      "   return argument\n",
      "------------------------ declaration ----------------------------------\n",
      "def encoding(argument):\n",
      " 'Verfies the encoding argument by lookup.\n",
      " (Directive option conversion function.)\n",
      " Raises ValueError for unknown encodings.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   for (k, v) in opts.items():\n",
      "      setConfigOption(k, v)\n",
      "------------------------ declaration ----------------------------------\n",
      "def setConfigOptions(**opts):\n",
      " 'Set global configuration options.\n",
      " Each keyword argument sets one global option.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   subKeys = itemName.split(':')\n",
      "   subResults = results\n",
      "   for subKey in subKeys:\n",
      "      subResults = subResults[subKey]\n",
      "   return subResults\n",
      "------------------------ declaration ----------------------------------\n",
      "def _getReportItem(itemName, results):\n",
      " 'Get a specific item by name out of the results dict.\n",
      " The format of itemName is a string of dictionary keys separated by colons,\n",
      " each key being one level deeper into the results dict. For example,\n",
      " \\'key1:key2\\' would fetch results[\\'key1\\'][\\'key2\\'].\n",
      " If itemName is not found in results, then None is returned'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   inner_text = []\n",
      "   for child in node.childNodes:\n",
      "      if ((child.nodeType == child.TEXT_NODE) or (child.nodeType == child.CDATA_SECTION_NODE)):\n",
      "         inner_text.append(child.data)\n",
      "      elif (child.nodeType == child.ELEMENT_NODE):\n",
      "         inner_text.extend(getInnerText(child))\n",
      "      else:\n",
      "         pass\n",
      "   return ''.join(inner_text)\n",
      "------------------------ declaration ----------------------------------\n",
      "def getInnerText(node):\n",
      " 'Get all the inner text of a DOM node (recursively).'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (not name):\n",
      "      return namespace(namespace_)\n",
      "   name = canonicalize_name(name)\n",
      "   if (name[0] == SEP):\n",
      "      resolved_name = name\n",
      "   elif is_private(name):\n",
      "      resolved_name = canonicalize_name(((namespace_ + SEP) + name[1:]))\n",
      "   else:\n",
      "      resolved_name = (namespace(namespace_) + name)\n",
      "   if (remappings and (resolved_name in remappings)):\n",
      "      return remappings[resolved_name]\n",
      "   else:\n",
      "      return resolved_name\n",
      "------------------------ declaration ----------------------------------\n",
      "def resolve_name(name, namespace_, remappings=None):\n",
      " 'Resolve a ROS name to its global, canonical form. Private ~names\n",
      " are resolved relative to the node name.\n",
      " @param name: name to resolve.\n",
      " @type  name: str\n",
      " @param namespace_: node name to resolve relative to.\n",
      " @type  namespace_: str\n",
      " @param remappings: Map of resolved remappings. Use None to indicate no remapping.\n",
      " @return: Resolved name. If name is empty/None, resolve_name\n",
      " returns parent namespace_. If namespace_ is empty/None,\n",
      " @rtype: str'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   qt_start = u'<!--:'\n",
      "   qt_end = u'-->'\n",
      "   qt_end_with_lang_len = 5\n",
      "   qt_chunks = text.split(qt_start)\n",
      "   content_by_lang = {}\n",
      "   common_txt_list = []\n",
      "   for c in qt_chunks:\n",
      "      if (not c.strip()):\n",
      "         continue\n",
      "      if c.startswith(qt_end):\n",
      "         lang = u''\n",
      "         c = c.lstrip(qt_end)\n",
      "         if (not c):\n",
      "            continue\n",
      "      elif c[2:].startswith(qt_end):\n",
      "         lang = c[:2]\n",
      "         c = c[qt_end_with_lang_len:]\n",
      "      else:\n",
      "         lang = u''\n",
      "      if (not lang):\n",
      "         common_txt_list.append(c)\n",
      "         for l in content_by_lang.keys():\n",
      "            content_by_lang[l].append(c)\n",
      "      else:\n",
      "         content_by_lang[lang] = (content_by_lang.get(lang, common_txt_list) + [c])\n",
      "   if (common_txt_list and (not content_by_lang)):\n",
      "      content_by_lang[u''] = common_txt_list\n",
      "   for l in content_by_lang.keys():\n",
      "      content_by_lang[l] = u'   '.join(content_by_lang[l])\n",
      "   return content_by_lang\n",
      "------------------------ declaration ----------------------------------\n",
      "def separate_qtranslate_content(text):\n",
      " 'Parse the content of a wordpress post or page and separate qtranslate languages.\n",
      " qtranslate tags: <!--:LL-->blabla<!--:-->'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   def getter(value):\n",
      "      if ET.iselement(value):\n",
      "         value = value.attrib\n",
      "      try:\n",
      "         if _is_re_match(value):\n",
      "            return value.group(item)\n",
      "         else:\n",
      "            return value[item]\n",
      "      except (KeyError, IndexError):\n",
      "         return default\n",
      "      except (TypeError, AttributeError) as err:\n",
      "         raise ValueError(err)\n",
      "   return transform(getter)\n",
      "------------------------ declaration ----------------------------------\n",
      "def get(item, default=None):\n",
      " 'Get item from value (value[item]).\n",
      " If the item is not found, return the default.\n",
      " Handles XML elements, regex matches and anything that has __getitem__.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   txt = txt.replace('\\r\\n', '\\n')\n",
      "   txt = txt.replace('\\r', '\\n')\n",
      "   txt_line_count = len(re.findall('(?mu)^\\\\s*.+$', txt))\n",
      "   docanalysis = DocAnalysis('txt', txt)\n",
      "   hardbreaks = docanalysis.line_histogram(0.55)\n",
      "   if hardbreaks:\n",
      "      tab_line_count = len(re.findall('(?mu)^( DCTB |\\\\s{2,}).+$', txt))\n",
      "      print_percent = (tab_line_count / float(txt_line_count))\n",
      "      empty_line_count = len(re.findall('(?mu)^\\\\s*$', txt))\n",
      "      block_percent = (empty_line_count / float(txt_line_count))\n",
      "      if (print_percent >= block_percent):\n",
      "         if (0.15 <= print_percent <= 0.75):\n",
      "            return 'print'\n",
      "      elif (0.15 <= block_percent <= 0.75):\n",
      "         return 'block'\n",
      "      return 'unformatted'\n",
      "   return 'single'\n",
      "------------------------ declaration ----------------------------------\n",
      "def detect_paragraph_type(txt):\n",
      " 'Tries to determine the paragraph type of the document.\n",
      " block: Paragraphs are separated by a blank line.\n",
      " single: Each line is a paragraph.\n",
      " print: Each paragraph starts with a 2+ spaces or a tab\n",
      " and ends when a new paragraph is reached.\n",
      " unformatted: most lines have hard line breaks, few/no blank lines or indents\n",
      " returns block, single, print, unformatted'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   idcount = 0\n",
      "   if (extension and (not extension.startswith('.'))):\n",
      "      extension = ('.%s' % extension)\n",
      "   fname = (base_name + extension)\n",
      "   while os.path.exists(fname):\n",
      "      fname = ('%s-%d%s' % (base_name, idcount, extension))\n",
      "      idcount += 1\n",
      "   return fname\n",
      "------------------------ declaration ----------------------------------\n",
      "def unique_file_name(base_name, extension=''):\n",
      " 'Creates a unique file name based on the specified base name.\n",
      " @base_name - The base name to use for the unique file name.\n",
      " @extension - The file extension to use for the unique file name.\n",
      " Returns a unique file string.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   so_far = set()\n",
      "   def no_dups(x):\n",
      "      k = key(x)\n",
      "      if (k in so_far):\n",
      "         return False\n",
      "      else:\n",
      "         so_far.add(k)\n",
      "         return True\n",
      "   return IteratorFilter(iterator, no_dups)\n",
      "------------------------ declaration ----------------------------------\n",
      "def UniqueIterator(iterator, key=(lambda x: x)):\n",
      " 'Takes an iterator and returns an iterator that returns only the\n",
      " first occurence of each entry'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (not is_list_like(n)):\n",
      "      n = np.array(([n] * len(m)))\n",
      "   elif (isinstance(n, np.ndarray) and (n.ndim == 0)):\n",
      "      n = np.repeat(np.array(n, ndmin=1), len(m))\n",
      "   try:\n",
      "      nn = n[m]\n",
      "      if (not _is_na_compat(v, nn[0])):\n",
      "         raise ValueError\n",
      "      nn_at = nn.astype(v.dtype)\n",
      "      if (not is_numeric_v_string_like(nn, nn_at)):\n",
      "         comp = (nn == nn_at)\n",
      "         if (is_list_like(comp) and comp.all()):\n",
      "            nv = v.copy()\n",
      "            nv[m] = nn_at\n",
      "            return nv\n",
      "   except (ValueError, IndexError, TypeError):\n",
      "      pass\n",
      "   (dtype, _) = _maybe_promote(n.dtype)\n",
      "   if (is_extension_type(v.dtype) and is_object_dtype(dtype)):\n",
      "      nv = v.get_values(dtype)\n",
      "   else:\n",
      "      nv = v.astype(dtype)\n",
      "   try:\n",
      "      nv[m] = n[m]\n",
      "   except ValueError:\n",
      "      (idx,) = np.where(np.squeeze(m))\n",
      "      for (mask_index, new_val) in zip(idx, n[m]):\n",
      "         nv[mask_index] = new_val\n",
      "   return nv\n",
      "------------------------ declaration ----------------------------------\n",
      "def _putmask_smart(v, m, n):\n",
      " 'Return a new block, try to preserve dtype if possible.\n",
      " Parameters\n",
      " v : `values`, updated in-place (array like)\n",
      " m : `mask`, applies to both sides (array like)\n",
      " n : `new values` either scalar or an array like aligned with `values`'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   filter_func = (filter_func or (lambda __: True))\n",
      "   stack = deque([start_node])\n",
      "   yield_results = {}\n",
      "   while stack:\n",
      "      current_node = stack.pop()\n",
      "      if (get_parents and (current_node != start_node)):\n",
      "         parents = get_parents(current_node)\n",
      "         if (not all(((parent in yield_results) for parent in parents))):\n",
      "            continue\n",
      "         elif ((not yield_descendants_of_unyielded) and (not any((yield_results[parent] for parent in parents)))):\n",
      "            continue\n",
      "      if (current_node not in yield_results):\n",
      "         if get_parents:\n",
      "            unvisited_children = list(get_children(current_node))\n",
      "         else:\n",
      "            unvisited_children = list((child for child in get_children(current_node) if (child not in yield_results)))\n",
      "         unvisited_children.reverse()\n",
      "         stack.extend(unvisited_children)\n",
      "         should_yield_node = filter_func(current_node)\n",
      "         if should_yield_node:\n",
      "            (yield current_node)\n",
      "         yield_results[current_node] = should_yield_node\n",
      "------------------------ declaration ----------------------------------\n",
      "def _traverse_generic(start_node, get_parents, get_children, filter_func=None, yield_descendants_of_unyielded=False):\n",
      " 'Helper function to avoid duplicating functionality between\n",
      " traverse_depth_first and traverse_topologically.\n",
      " If get_parents is None, do a pre-order traversal.\n",
      " Else, do a topological traversal.\n",
      " The topological traversal has a worse time complexity than\n",
      " pre-order does, as it needs to check whether each node\\'s\n",
      " parents have been visited.\n",
      " Arguments:\n",
      " See description in traverse_topologically.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   end = evaluate.getVector3FromElementNode(elementNode)\n",
      "   previousElementNode = elementNode.getPreviousElementNode()\n",
      "   if (previousElementNode == None):\n",
      "      print 'Warning,   can   not   get   previousElementNode   in   getQuadraticPath   in   quadratic   for:'\n",
      "      print elementNode\n",
      "      return [end]\n",
      "   begin = elementNode.getPreviousVertex(Vector3())\n",
      "   controlPoint = evaluate.getVector3ByPrefix(None, elementNode, 'controlPoint')\n",
      "   if (controlPoint == None):\n",
      "      oldControlPoint = evaluate.getVector3ByPrefixes(previousElementNode, ['controlPoint', 'controlPoint1'], None)\n",
      "      if (oldControlPoint == None):\n",
      "         oldControlPoint = end\n",
      "      controlPoint = ((begin + begin) - oldControlPoint)\n",
      "      evaluate.addVector3ToElementNode(elementNode, 'controlPoint', controlPoint)\n",
      "   return svg_reader.getQuadraticPoints(begin, controlPoint, end, lineation.getNumberOfBezierPoints(begin, elementNode, end))\n",
      "------------------------ declaration ----------------------------------\n",
      "def getQuadraticPath(elementNode):\n",
      " 'Get the quadratic path.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (isinstance(node.op, tensor.Elemwise) and isinstance(node.op.scalar_op, scalar.basic.Log) and (len(node.inputs) == 1) and (node.inputs[0].owner is not None) and isinstance(node.inputs[0].owner.op, Softmax)):\n",
      "      inVars = node.inputs[0].owner.inputs[0]\n",
      "      new_op = LogSoftmax()\n",
      "      ret = new_op(inVars)\n",
      "      ret.tag.values_eq_approx = values_eq_approx_remove_inf\n",
      "      copy_stack_trace([node.inputs[0], node.outputs[0]], ret)\n",
      "      return [ret]\n",
      "------------------------ declaration ----------------------------------\n",
      "@opt.register_specialize('stabilize', 'fast_compile')\n",
      " @gof.local_optimizer([tensor.Elemwise])\n",
      " def local_logsoftmax(node):\n",
      " 'Detect Log(Softmax(x)) and replace it with LogSoftmax(x)\n",
      " Note: only forward pass is affected'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   freqs = numpy.array([(((f + 1) * fs) / (2 * nfft)) for f in range(nfft)])\n",
      "   Cp = 27.5\n",
      "   nChroma = numpy.round((12.0 * numpy.log2((freqs / Cp)))).astype(int)\n",
      "   nFreqsPerChroma = numpy.zeros((nChroma.shape[0],))\n",
      "   uChroma = numpy.unique(nChroma)\n",
      "   for u in uChroma:\n",
      "      idx = numpy.nonzero((nChroma == u))\n",
      "      nFreqsPerChroma[idx] = idx[0].shape\n",
      "   return (nChroma, nFreqsPerChroma)\n",
      "------------------------ declaration ----------------------------------\n",
      "def stChromaFeaturesInit(nfft, fs):\n",
      " 'This function initializes the chroma matrices used in the calculation of the chroma features'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   def _set_nxm_headers_dec(self):\n",
      "      self.nxm_headers = nxm_headers\n",
      "      return self\n",
      "   return _set_nxm_headers_dec\n",
      "------------------------ declaration ----------------------------------\n",
      "def _set_nxm_headers(nxm_headers):\n",
      " 'Annotate corresponding NXM header'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return get_i18n().gettext(string, **variables)\n",
      "------------------------ declaration ----------------------------------\n",
      "def gettext(string, **variables):\n",
      " 'See :meth:`I18n.gettext`.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   diff = (X - x_old)\n",
      "   diff_norm = np.sqrt(np.sum((diff ** 2), axis=1))\n",
      "   mask = (diff_norm >= _EPSILON)\n",
      "   is_x_old_in_X = int((mask.sum() < X.shape[0]))\n",
      "   diff = diff[mask]\n",
      "   diff_norm = diff_norm[mask][:, np.newaxis]\n",
      "   quotient_norm = linalg.norm(np.sum((diff / diff_norm), axis=0))\n",
      "   if (quotient_norm > _EPSILON):\n",
      "      new_direction = (np.sum((X[mask, :] / diff_norm), axis=0) / np.sum((1 / diff_norm), axis=0))\n",
      "   else:\n",
      "      new_direction = 1.0\n",
      "      quotient_norm = 1.0\n",
      "   return ((max(0.0, (1.0 - (is_x_old_in_X / quotient_norm))) * new_direction) + (min(1.0, (is_x_old_in_X / quotient_norm)) * x_old))\n",
      "------------------------ declaration ----------------------------------\n",
      "def _modified_weiszfeld_step(X, x_old):\n",
      " 'Modified Weiszfeld step.\n",
      " This function defines one iteration step in order to approximate the\n",
      " spatial median (L1 median). It is a form of an iteratively re-weighted\n",
      " least squares method.\n",
      " Parameters\n",
      " X : array, shape = [n_samples, n_features]\n",
      " Training vector, where n_samples is the number of samples and\n",
      " n_features is the number of features.\n",
      " x_old : array, shape = [n_features]\n",
      " Current start vector.\n",
      " Returns\n",
      " x_new : array, shape = [n_features]\n",
      " New iteration step.\n",
      " References\n",
      " - On Computation of Spatial Median for Robust Data Mining, 2005\n",
      " T. Kärkkäinen and S. Äyrämö\n",
      " http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (not hasattr(req, 'best_match_language')):\n",
      "      return e\n",
      "   locale = req.best_match_language()\n",
      "   if isinstance(e, webob.exc.HTTPError):\n",
      "      e.explanation = i18n.translate(e.explanation, locale)\n",
      "      e.detail = i18n.translate(e.detail, locale)\n",
      "      if getattr(e, 'body_template', None):\n",
      "         e.body_template = i18n.translate(e.body_template, locale)\n",
      "   return e\n",
      "------------------------ declaration ----------------------------------\n",
      "def translate_exception(req, e):\n",
      " 'Translates all translatable elements of the given exception.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return uuid.uuid4().hex\n",
      "------------------------ declaration ----------------------------------\n",
      "def rand_uuid_hex():\n",
      " 'Generate a random UUID hex string\n",
      " :return: a random UUID (e.g. \\'0b98cf96d90447bda4b46f31aeb1508c\\')\n",
      " :rtype: string'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return time.time()\n",
      "------------------------ declaration ----------------------------------\n",
      "def current_time():\n",
      " 'Retrieve the current time, this function is mocked out in unit testing.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   would_be_changed = []\n",
      "   for package in packages:\n",
      "      (installed, updated) = query_package(module, xbps_path, package)\n",
      "      if (((state in ['present', 'latest']) and (not installed)) or ((state == 'absent') and installed) or ((state == 'latest') and (not updated))):\n",
      "         would_be_changed.append(package)\n",
      "   if would_be_changed:\n",
      "      if (state == 'absent'):\n",
      "         state = 'removed'\n",
      "      module.exit_json(changed=True, msg=('%s   package(s)   would   be   %s' % (len(would_be_changed), state)), packages=would_be_changed)\n",
      "   else:\n",
      "      module.exit_json(changed=False, msg=('package(s)   already   %s' % state), packages=[])\n",
      "------------------------ declaration ----------------------------------\n",
      "def check_packages(module, xbps_path, packages, state):\n",
      " 'Returns change status of command'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   for d in frappe.get_all(u'Daily   Work   Summary', dict(status=u'Open')):\n",
      "      daily_work_summary = frappe.get_doc(u'Daily   Work   Summary', d.name)\n",
      "      daily_work_summary.send_summary()\n",
      "------------------------ declaration ----------------------------------\n",
      "def send_summary():\n",
      " 'Send summary to everyone'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (provider in drivers):\n",
      "      raise AttributeError(('Provider   %s   already   registered' % provider))\n",
      "   drivers[provider] = (module, klass)\n",
      "   try:\n",
      "      driver = get_driver(drivers, provider)\n",
      "   except (ImportError, AttributeError):\n",
      "      exp = sys.exc_info()[1]\n",
      "      drivers.pop(provider)\n",
      "      raise exp\n",
      "   return driver\n",
      "------------------------ declaration ----------------------------------\n",
      "def set_driver(drivers, provider, module, klass):\n",
      " 'Sets a driver.\n",
      " :param drivers: Dictionary to store providers.\n",
      " :param provider: Id of provider to set driver for\n",
      " :type provider: :class:`libcloud.types.Provider`\n",
      " :param module: The module which contains the driver\n",
      " :type module: L\n",
      " :param klass: The driver class name\n",
      " :type klass:'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return str(tablename)\n",
      "------------------------ declaration ----------------------------------\n",
      "def classname_for_table(base, tablename, table):\n",
      " 'Return the class name that should be used, given the name\n",
      " of a table.\n",
      " The default implementation is::\n",
      " return str(tablename)\n",
      " Alternate implementations can be specified using the\n",
      " :paramref:`.AutomapBase.prepare.classname_for_table`\n",
      " parameter.\n",
      " :param base: the :class:`.AutomapBase` class doing the prepare.\n",
      " :param tablename: string name of the :class:`.Table`.\n",
      " :param table: the :class:`.Table` object itself.\n",
      " :return: a string class name.\n",
      " .. note::\n",
      " In Python 2, the string used for the class name **must** be a non-Unicode\n",
      " object, e.g. a ``str()`` object.  The ``.name`` attribute of\n",
      " :class:`.Table` is typically a Python unicode subclass, so the ``str()``\n",
      " function should be applied to this name, after accounting for any non-ASCII\n",
      " characters.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   splitLine = line.split()\n",
      "   return Vector3(float(splitLine[1]), float(splitLine[2]), float(splitLine[3]))\n",
      "------------------------ declaration ----------------------------------\n",
      "def getVertexGivenLine(line):\n",
      " 'Get vertex given obj vertex line.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   flags = []\n",
      "   TCP_FIN = 1\n",
      "   TCP_SYN = 2\n",
      "   TCP_RST = 4\n",
      "   TCP_PSH = 8\n",
      "   TCP_ACK = 16\n",
      "   TCP_URG = 32\n",
      "   TCP_ECE = 64\n",
      "   TCP_CWK = 128\n",
      "   if (packet['tcp']['flags'] & TCP_FIN):\n",
      "      flags.append('FIN')\n",
      "   elif (packet['tcp']['flags'] & TCP_SYN):\n",
      "      flags.append('SYN')\n",
      "   elif (packet['tcp']['flags'] & TCP_RST):\n",
      "      flags.append('RST')\n",
      "   elif (packet['tcp']['flags'] & TCP_PSH):\n",
      "      flags.append('PSH')\n",
      "   elif (packet['tcp']['flags'] & TCP_ACK):\n",
      "      flags.append('ACK')\n",
      "   elif (packet['tcp']['flags'] & TCP_URG):\n",
      "      flags.append('URG')\n",
      "   elif (packet['tcp']['flags'] & TCP_ECE):\n",
      "      flags.append('ECE')\n",
      "   elif (packet['tcp']['flags'] & TCP_CWK):\n",
      "      flags.append('CWK')\n",
      "   else:\n",
      "      print('UNKNOWN   PACKET')\n",
      "   if (packet['tcp']['d_port'] == 4505):\n",
      "      if (('SYN' in flags) and (len(flags) == 1)):\n",
      "         return 10\n",
      "      elif ('FIN' in flags):\n",
      "         return 12\n",
      "   elif (packet['tcp']['d_port'] == 4506):\n",
      "      if (('SYN' in flags) and (len(flags) == 1)):\n",
      "         return 100\n",
      "      elif ('FIN' in flags):\n",
      "         return 120\n",
      "   else:\n",
      "      return None\n",
      "------------------------ declaration ----------------------------------\n",
      "def filter_new_cons(packet):\n",
      " 'filter packets by there tcp-state and\n",
      " returns codes for specific states'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   keys = []\n",
      "   for i in request['item']:\n",
      "      if ('MemcacheSetRequest_Item' in i):\n",
      "         key = i['MemcacheSetRequest_Item']['key']\n",
      "      else:\n",
      "         key = i['Item']['key']\n",
      "      keys.append(truncate(key))\n",
      "   return '\\n'.join(keys)\n",
      "------------------------ declaration ----------------------------------\n",
      "def memcache_set(request):\n",
      " 'Pretty-format a memcache.set() request.\n",
      " Arguments:\n",
      " request - The memcache.set() request object, e.g.,\n",
      " {\\'item\\': [{\\'Item\\': {\\'flags\\': \\'0L\\', \\'key\\': \\'memcache_key\\' ...\n",
      " Returns:\n",
      " The keys of the memcache.get() response as a string. If there are\n",
      " multiple keys, they are separated by newline characters.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if isinstance(tck, BSpline):\n",
      "      return tck.antiderivative(n)\n",
      "   else:\n",
      "      return _impl.splantider(tck, n)\n",
      "------------------------ declaration ----------------------------------\n",
      "def splantider(tck, n=1):\n",
      " 'Compute the spline for the antiderivative (integral) of a given spline.\n",
      " Parameters\n",
      " tck : BSpline instance or a tuple of (t, c, k)\n",
      " Spline whose antiderivative to compute\n",
      " n : int, optional\n",
      " Order of antiderivative to evaluate. Default: 1\n",
      " Returns\n",
      " BSpline instance or a tuple of (t2, c2, k2)\n",
      " Spline of order k2=k+n representing the antiderivative of the input\n",
      " spline.\n",
      " A tuple is returned iff the input argument `tck` is a tuple, otherwise\n",
      " a BSpline object is constructed and returned.\n",
      " See Also\n",
      " splder, splev, spalde\n",
      " BSpline\n",
      " Notes\n",
      " The `splder` function is the inverse operation of this function.\n",
      " Namely, ``splder(splantider(tck))`` is identical to `tck`, modulo\n",
      " rounding error.\n",
      " .. versionadded:: 0.13.0\n",
      " Examples\n",
      " >>> from scipy.interpolate import splrep, splder, splantider, splev\n",
      " >>> x = np.linspace(0, np.pi/2, 70)\n",
      " >>> y = 1 / np.sqrt(1 - 0.8*np.sin(x)**2)\n",
      " >>> spl = splrep(x, y)\n",
      " The derivative is the inverse operation of the antiderivative,\n",
      " although some floating point error accumulates:\n",
      " >>> splev(1.7, spl), splev(1.7, splder(splantider(spl)))\n",
      " (array(2.1565429877197317), array(2.1565429877201865))\n",
      " Antiderivative can be used to evaluate definite integrals:\n",
      " >>> ispl = splantider(spl)\n",
      " >>> splev(np.pi/2, ispl) - splev(0, ispl)\n",
      " 2.2572053588768486\n",
      " This is indeed an approximation to the complete elliptic integral\n",
      " :math:`K(m) = \\int_0^{\\pi/2} [1 - m\\sin^2 x]^{-1/2} dx`:\n",
      " >>> from scipy.special import ellipk\n",
      " >>> ellipk(0.8)\n",
      " 2.2572053268208538'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if tag:\n",
      "      return tag(context, objects)\n",
      "   request = context['request']\n",
      "   response_format = 'html'\n",
      "   if ('response_format' in context):\n",
      "      response_format = context['response_format']\n",
      "   return Markup(render_to_string('core/tags/generic_list', {'objects': objects, 'skip_group': skip_group}, context_instance=RequestContext(request), response_format=response_format))\n",
      "------------------------ declaration ----------------------------------\n",
      "@contextfunction\n",
      " def core_generic_list(context, objects, skip_group=False, tag=None):\n",
      " 'Print a list of objects'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if pf_interface:\n",
      "      return ('/sys/bus/pci/devices/%s/physfn/net' % pci_addr)\n",
      "   return ('/sys/bus/pci/devices/%s/net' % pci_addr)\n",
      "------------------------ declaration ----------------------------------\n",
      "def _get_sysfs_netdev_path(pci_addr, pf_interface):\n",
      " 'Get the sysfs path based on the PCI address of the device.\n",
      " Assumes a networking device - will not check for the existence of the path.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   t = desc.type\n",
      "   if unpacker_coercions.has_key(t):\n",
      "      desc = desc.AECoerceDesc(unpacker_coercions[t])\n",
      "      t = desc.type\n",
      "   if (t == typeAEList):\n",
      "      l = []\n",
      "      for i in range(desc.AECountItems()):\n",
      "         (keyword, item) = desc.AEGetNthDesc((i + 1), '****')\n",
      "         l.append(unpack(item, formodulename))\n",
      "      return l\n",
      "   if (t == typeAERecord):\n",
      "      d = {}\n",
      "      for i in range(desc.AECountItems()):\n",
      "         (keyword, item) = desc.AEGetNthDesc((i + 1), '****')\n",
      "         d[keyword] = unpack(item, formodulename)\n",
      "      return d\n",
      "   if (t == typeAEText):\n",
      "      record = desc.AECoerceDesc('reco')\n",
      "      return mkaetext(unpack(record, formodulename))\n",
      "   if (t == typeAlias):\n",
      "      return Carbon.File.Alias(rawdata=desc.data)\n",
      "   if (t == typeBoolean):\n",
      "      return struct.unpack('b', desc.data)[0]\n",
      "   if (t == typeChar):\n",
      "      return desc.data\n",
      "   if (t == typeUnicodeText):\n",
      "      return unicode(desc.data, 'utf16')\n",
      "   if (t == typeEnumeration):\n",
      "      return mkenum(desc.data)\n",
      "   if (t == typeFalse):\n",
      "      return 0\n",
      "   if (t == typeFloat):\n",
      "      data = desc.data\n",
      "      return struct.unpack('d', data)[0]\n",
      "   if (t == typeFSS):\n",
      "      return Carbon.File.FSSpec(rawdata=desc.data)\n",
      "   if (t == typeFSRef):\n",
      "      return Carbon.File.FSRef(rawdata=desc.data)\n",
      "   if (t == typeInsertionLoc):\n",
      "      record = desc.AECoerceDesc('reco')\n",
      "      return mkinsertionloc(unpack(record, formodulename))\n",
      "   if (t == typeIntlText):\n",
      "      (script, language) = struct.unpack('hh', desc.data[:4])\n",
      "      return aetypes.IntlText(script, language, desc.data[4:])\n",
      "   if (t == typeIntlWritingCode):\n",
      "      (script, language) = struct.unpack('hh', desc.data)\n",
      "      return aetypes.IntlWritingCode(script, language)\n",
      "   if (t == typeKeyword):\n",
      "      return mkkeyword(desc.data)\n",
      "   if (t == typeLongInteger):\n",
      "      return struct.unpack('l', desc.data)[0]\n",
      "   if (t == typeLongDateTime):\n",
      "      (a, b) = struct.unpack('lL', desc.data)\n",
      "      return ((long(a) << 32) + b)\n",
      "   if (t == typeNull):\n",
      "      return None\n",
      "   if (t == typeMagnitude):\n",
      "      v = struct.unpack('l', desc.data)\n",
      "      if (v < 0):\n",
      "         v = (4294967296L + v)\n",
      "      return v\n",
      "   if (t == typeObjectSpecifier):\n",
      "      record = desc.AECoerceDesc('reco')\n",
      "      if formodulename:\n",
      "         return mkobjectfrommodule(unpack(record, formodulename), formodulename)\n",
      "      return mkobject(unpack(record, formodulename))\n",
      "   if (t == typeQDPoint):\n",
      "      (v, h) = struct.unpack('hh', desc.data)\n",
      "      return aetypes.QDPoint(v, h)\n",
      "   if (t == typeQDRectangle):\n",
      "      (v0, h0, v1, h1) = struct.unpack('hhhh', desc.data)\n",
      "      return aetypes.QDRectangle(v0, h0, v1, h1)\n",
      "   if (t == typeRGBColor):\n",
      "      (r, g, b) = struct.unpack('hhh', desc.data)\n",
      "      return aetypes.RGBColor(r, g, b)\n",
      "   if (t == typeShortFloat):\n",
      "      return struct.unpack('f', desc.data)[0]\n",
      "   if (t == typeShortInteger):\n",
      "      return struct.unpack('h', desc.data)[0]\n",
      "   if (t == typeTargetID):\n",
      "      return mktargetid(desc.data)\n",
      "   if (t == typeTrue):\n",
      "      return 1\n",
      "   if (t == typeType):\n",
      "      return mktype(desc.data, formodulename)\n",
      "   if (t == 'rang'):\n",
      "      record = desc.AECoerceDesc('reco')\n",
      "      return mkrange(unpack(record, formodulename))\n",
      "   if (t == 'cmpd'):\n",
      "      record = desc.AECoerceDesc('reco')\n",
      "      return mkcomparison(unpack(record, formodulename))\n",
      "   if (t == 'logi'):\n",
      "      record = desc.AECoerceDesc('reco')\n",
      "      return mklogical(unpack(record, formodulename))\n",
      "   return mkunknown(desc.type, desc.data)\n",
      "------------------------ declaration ----------------------------------\n",
      "def unpack(desc, formodulename=''):\n",
      " 'Unpack an AE descriptor to a python object'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return [transl[l] for l in labels]\n",
      "------------------------ declaration ----------------------------------\n",
      "def convert_labels(labels, transl):\n",
      " 'Convert between strings and numbers.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   mask = ','.join(mask)\n",
      "   ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''}\n",
      "   if __opts__['test']:\n",
      "      status = _check_cron(user, path, mask, cmd)\n",
      "      ret['result'] = None\n",
      "      if (status == 'absent'):\n",
      "         ret['result'] = True\n",
      "         ret['comment'] = 'Incron   {0}   is   absent'.format(name)\n",
      "      elif ((status == 'present') or (status == 'update')):\n",
      "         ret['comment'] = 'Incron   {0}   is   set   to   be   removed'.format(name)\n",
      "      return ret\n",
      "   data = __salt__['incron.rm_job'](user=user, path=path, mask=mask, cmd=cmd)\n",
      "   if (data == 'absent'):\n",
      "      ret['comment'] = 'Incron   {0}   already   absent'.format(name)\n",
      "      return ret\n",
      "   if (data == 'removed'):\n",
      "      ret['comment'] = \"Incron   {0}   removed   from   {1}'s   crontab\".format(name, user)\n",
      "      ret['changes'] = {user: name}\n",
      "      return ret\n",
      "   ret['comment'] = 'Incron   {0}   for   user   {1}   failed   to   commit   with   error   {2}'.format(name, user, data)\n",
      "   ret['result'] = False\n",
      "   return ret\n",
      "------------------------ declaration ----------------------------------\n",
      "def absent(name, path, mask, cmd, user='root'):\n",
      " 'Verifies that the specified incron job is absent for the specified user; only\n",
      " the name is matched when removing a incron job.\n",
      " name\n",
      " Unique comment describing the entry\n",
      " path\n",
      " The path that should be watched\n",
      " user\n",
      " The name of the user who\\'s crontab needs to be modified, defaults to\n",
      " the root user\n",
      " mask\n",
      " The mask of events that should be monitored for\n",
      " cmd\n",
      " The cmd that should be executed'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return _list_users()\n",
      "------------------------ declaration ----------------------------------\n",
      "@blueprint.route('/users')\n",
      " def list_all_users():\n",
      " 'Return a list of all known user names.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   import inspect\n",
      "   if isinstance(frame, six.string_types):\n",
      "      frame_names = frame_transform_graph.get_names()\n",
      "      if (frame not in frame_names):\n",
      "         raise ValueError(u'Coordinate   frame   {0}   not   in   allowed   values   {1}'.format(frame, sorted(frame_names)))\n",
      "      frame_cls = frame_transform_graph.lookup_name(frame)\n",
      "   elif (inspect.isclass(frame) and issubclass(frame, BaseCoordinateFrame)):\n",
      "      frame_cls = frame\n",
      "   else:\n",
      "      raise ValueError(u'Coordinate   frame   must   be   a   frame   name   or   frame   class')\n",
      "   return frame_cls\n",
      "------------------------ declaration ----------------------------------\n",
      "def _get_frame_class(frame):\n",
      " 'Get a frame class from the input `frame`, which could be a frame name\n",
      " string, or frame class.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   while u:\n",
      "      f = dmp_TC(f, K)\n",
      "      u -= 1\n",
      "   return dup_TC(f, K)\n",
      "------------------------ declaration ----------------------------------\n",
      "def dmp_ground_TC(f, u, K):\n",
      " 'Return the ground trailing coefficient.\n",
      " Examples\n",
      " >>> from sympy.polys.domains import ZZ\n",
      " >>> from sympy.polys.densebasic import dmp_ground_TC\n",
      " >>> f = ZZ.map([[[1], [2, 3]]])\n",
      " >>> dmp_ground_TC(f, 2, ZZ)\n",
      " 3'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   cur_chromosome = BasicChromosome.Chromosome(chr_name)\n",
      "   num_segments = random.randrange(num_possible_segments)\n",
      "   for seg in range(num_segments):\n",
      "      if (seg == 0):\n",
      "         cur_segment = BasicChromosome.TelomereSegment()\n",
      "      elif (seg == (num_segments - 1)):\n",
      "         cur_segment = BasicChromosome.TelomereSegment(1)\n",
      "      else:\n",
      "         cur_segment = BasicChromosome.ChromosomeSegment()\n",
      "      color_chance = random.random()\n",
      "      if (color_chance <= color_prob):\n",
      "         fill_color = random.choice(color_choices)\n",
      "         cur_segment.fill_color = fill_color\n",
      "      id_chance = random.random()\n",
      "      if (id_chance <= id_prob):\n",
      "         id = get_random_id()\n",
      "         cur_segment.label = id\n",
      "      cur_chromosome.add(cur_segment)\n",
      "   return (cur_chromosome, num_segments)\n",
      "------------------------ declaration ----------------------------------\n",
      "def load_random_chromosome(chr_name):\n",
      " 'Generate a chromosome with random information about it.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   imdb = datasets.pascal_voc(split, year)\n",
      "   imdb.roidb_handler = imdb.selective_search_IJCV_roidb\n",
      "   imdb.config['top_k'] = top_k\n",
      "   return imdb\n",
      "------------------------ declaration ----------------------------------\n",
      "def _selective_search_IJCV_top_k(split, year, top_k):\n",
      " 'Return an imdb that uses the top k proposals from the selective search\n",
      " IJCV code.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if opts['file_ignore_regex']:\n",
      "      for regex in opts['file_ignore_regex']:\n",
      "         if re.search(regex, fname):\n",
      "            log.debug('File   matching   file_ignore_regex.   Skipping:   {0}'.format(fname))\n",
      "            return True\n",
      "   if opts['file_ignore_glob']:\n",
      "      for glob in opts['file_ignore_glob']:\n",
      "         if fnmatch.fnmatch(fname, glob):\n",
      "            log.debug('File   matching   file_ignore_glob.   Skipping:   {0}'.format(fname))\n",
      "            return True\n",
      "   return False\n",
      "------------------------ declaration ----------------------------------\n",
      "def is_file_ignored(opts, fname):\n",
      " 'If file_ignore_regex or file_ignore_glob were given in config,\n",
      " compare the given file path against all of them and return True\n",
      " on the first match.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   msg = u'{0}   Assuming   UT1-UTC=0   for   coordinate   transformations.'\n",
      "   warnings.warn(msg.format(ierserr.args[0]), AstropyWarning)\n",
      "------------------------ declaration ----------------------------------\n",
      "def _warn_iers(ierserr):\n",
      " 'Generate a warning for an IERSRangeerror\n",
      " Parameters\n",
      " ierserr : An `~astropy.utils.iers.IERSRangeError`'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   axis = _normalize_axis(axis, ndim(x))\n",
      "   if (x.dtype.base_dtype == tf.bool):\n",
      "      x = tf.cast(x, floatx())\n",
      "   return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n",
      "------------------------ declaration ----------------------------------\n",
      "def mean(x, axis=None, keepdims=False):\n",
      " 'Mean of a tensor, alongside the specified axis.\n",
      " # Arguments\n",
      " x: A tensor or variable.\n",
      " axis: A list of integer. Axes to compute the mean.\n",
      " keepdims: A boolean, whether to keep the dimensions or not.\n",
      " If `keepdims` is `False`, the rank of the tensor is reduced\n",
      " by 1 for each entry in `axis`. If `keep_dims` is `True`,\n",
      " the reduced dimensions are retained with length 1.\n",
      " # Returns\n",
      " A tensor with the mean of elements of `x`.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   src_from = _ensure_src(src_from)\n",
      "   subject_from = _ensure_src_subject(src_from, subject_from)\n",
      "   subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)\n",
      "   src_out = list()\n",
      "   for fro in src_from:\n",
      "      (hemi, idx, id_) = _get_hemi(fro)\n",
      "      to = op.join(subjects_dir, subject_to, 'surf', ('%s.%s' % (hemi, surf)))\n",
      "      logger.info(('Reading   destination   surface   %s' % (to,)))\n",
      "      to = read_surface(to, return_dict=True, verbose=False)[(-1)]\n",
      "      complete_surface_info(to, copy=False)\n",
      "      best = _get_vertex_map_nn(fro, subject_from, subject_to, hemi, subjects_dir, to['neighbor_tri'])\n",
      "      for key in ('neighbor_tri', 'tri_area', 'tri_cent', 'tri_nn', 'use_tris'):\n",
      "         del to[key]\n",
      "      to['vertno'] = np.sort(best[fro['vertno']])\n",
      "      to['inuse'] = np.zeros(len(to['rr']), int)\n",
      "      to['inuse'][to['vertno']] = True\n",
      "      to['use_tris'] = best[fro['use_tris']]\n",
      "      to.update(nuse=len(to['vertno']), nuse_tri=len(to['use_tris']), nearest=None, nearest_dist=None, patch_inds=None, pinfo=None, dist=None, id=id_, dist_limit=None, type='surf', coord_frame=FIFF.FIFFV_COORD_MRI, subject_his_id=subject_to, rr=(to['rr'] / 1000.0))\n",
      "      src_out.append(to)\n",
      "      logger.info('[done]\\n')\n",
      "   info = dict(working_dir=os.getcwd(), command_line=_get_call_line(in_verbose=True))\n",
      "   return SourceSpaces(src_out, info=info)\n",
      "------------------------ declaration ----------------------------------\n",
      "@verbose\n",
      " def morph_source_spaces(src_from, subject_to, surf='white', subject_from=None, subjects_dir=None, verbose=None):\n",
      " 'Morph an existing source space to a different subject.\n",
      " .. warning:: This can be used in place of morphing source estimates for\n",
      " multiple subjects, but there may be consequences in terms\n",
      " of dipole topology.\n",
      " Parameters\n",
      " src_from : instance of SourceSpaces\n",
      " Surface source spaces to morph.\n",
      " subject_to : str\n",
      " The destination subject.\n",
      " surf : str\n",
      " The brain surface to use for the new source space.\n",
      " subject_from : str | None\n",
      " The \"from\" subject. For most source spaces this shouldn\\'t need\n",
      " to be provided, since it is stored in the source space itself.\n",
      " subjects_dir : string, or None\n",
      " Path to SUBJECTS_DIR if it is not set in the environment.\n",
      " verbose : bool, str, int, or None\n",
      " If not None, override default verbose level (see :func:`mne.verbose`\n",
      " and :ref:`Logging documentation <tut_logging>` for more).\n",
      " Returns\n",
      " src : instance of SourceSpaces\n",
      " The morphed source spaces.\n",
      " Notes\n",
      " .. versionadded:: 0.10.0'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   encoding = kwds.get('encoding', None)\n",
      "   if (encoding is not None):\n",
      "      encoding = re.sub('_', '-', encoding).lower()\n",
      "      kwds['encoding'] = encoding\n",
      "   compression = kwds.get('compression')\n",
      "   compression = _infer_compression(filepath_or_buffer, compression)\n",
      "   (filepath_or_buffer, _, compression) = get_filepath_or_buffer(filepath_or_buffer, encoding, compression)\n",
      "   kwds['compression'] = compression\n",
      "   if (kwds.get('date_parser', None) is not None):\n",
      "      if isinstance(kwds['parse_dates'], bool):\n",
      "         kwds['parse_dates'] = True\n",
      "   iterator = kwds.get('iterator', False)\n",
      "   chunksize = kwds.get('chunksize', None)\n",
      "   nrows = _validate_nrows(kwds.pop('nrows', None))\n",
      "   parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "   if ((nrows is not None) and (chunksize is not None)):\n",
      "      raise NotImplementedError(\"'nrows'   and   'chunksize'   cannot   be   used   together   yet.\")\n",
      "   elif (nrows is not None):\n",
      "      try:\n",
      "         data = parser.read(nrows)\n",
      "      finally:\n",
      "         parser.close()\n",
      "      return data\n",
      "   elif (chunksize or iterator):\n",
      "      return parser\n",
      "   try:\n",
      "      data = parser.read()\n",
      "   finally:\n",
      "      parser.close()\n",
      "   return data\n",
      "------------------------ declaration ----------------------------------\n",
      "def _read(filepath_or_buffer, kwds):\n",
      " 'Generic reader of line files.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   (tags, noserver) = read_etag_file(options)\n",
      "   if (noserver and (not options.noserver)):\n",
      "      options.noserver = noserver\n",
      "   m = re.match('(?:W/)?\"?(.*)\"?$', options.etag)\n",
      "   if m:\n",
      "      options.etag = m.group(1)\n",
      "   etag = options.etag\n",
      "   if (etag in tags):\n",
      "      print ('Found   etag   [%s]   for   version   %s' % (etag, tags[etag][0]['version']))\n",
      "      return tags[etag]\n",
      "   short = etag[etag.index('-'):]\n",
      "   for t in tags:\n",
      "      if (t.find(short) != (-1)):\n",
      "         print ('Partial   ETag   match:   [%s],[%s]   for   version   %s' % (etag, t, tags[t][0]['version']))\n",
      "         return tags[t]\n",
      "   return None\n",
      "------------------------ declaration ----------------------------------\n",
      "def get_details_for_etag(options):\n",
      " 'Get the stack address for a specific ETag from the configuration file.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (get_configured_provider() is False):\n",
      "      return False\n",
      "   if (get_dependencies() is False):\n",
      "      return False\n",
      "   for (provider, details) in six.iteritems(__opts__['providers']):\n",
      "      if ('dimensiondata' not in details):\n",
      "         continue\n",
      "   return __virtualname__\n",
      "------------------------ declaration ----------------------------------\n",
      "def __virtual__():\n",
      " 'Set up the libcloud functions and check for dimensiondata configurations.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   R = p.ring\n",
      "   n = R.gens.index(x)\n",
      "   p1 = R.zero\n",
      "   mn = ([0] * R.ngens)\n",
      "   mn[n] = 1\n",
      "   mn = tuple(mn)\n",
      "   for expv in p:\n",
      "      if expv[n]:\n",
      "         e = monomial_ldiv(expv, mn)\n",
      "         p1[e] = (p[expv] * expv[n])\n",
      "   return p1\n",
      "------------------------ declaration ----------------------------------\n",
      "def rs_diff(p, x):\n",
      " 'Return partial derivative of ``p`` with respect to ``x``.\n",
      " Parameters\n",
      " x : :class:`PolyElement` with respect to which ``p`` is differentiated.\n",
      " Examples\n",
      " >>> from sympy.polys.domains import QQ\n",
      " >>> from sympy.polys.rings import ring\n",
      " >>> from sympy.polys.ring_series import rs_diff\n",
      " >>> R, x, y = ring(\\'x, y\\', QQ)\n",
      " >>> p = x + x**2*y**3\n",
      " >>> rs_diff(p, x)\n",
      " 2*x*y**3 + 1'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (pkg in _bootstrapped):\n",
      "      return []\n",
      "   m = rospack.get_manifest(pkg)\n",
      "   if m.is_catkin:\n",
      "      _bootstrapped.append(pkg)\n",
      "      return []\n",
      "   packages = get_depends(pkg, rospack)\n",
      "   packages.append(pkg)\n",
      "   paths = []\n",
      "   try:\n",
      "      for p in packages:\n",
      "         m = rospack.get_manifest(p)\n",
      "         d = rospack.get_path(p)\n",
      "         _append_package_paths(m, paths, d)\n",
      "         _bootstrapped.append(p)\n",
      "   except:\n",
      "      if (pkg in _bootstrapped):\n",
      "         _bootstrapped.remove(pkg)\n",
      "      raise\n",
      "   return paths\n",
      "------------------------ declaration ----------------------------------\n",
      "def _generate_python_path(pkg, rospack):\n",
      " 'Recursive subroutine for building dependency list and python path\n",
      " :raises: :exc:`rospkg.ResourceNotFound` If an error occurs while attempting to load package or dependencies'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   (procs, pipes) = launch_tails(follow_paths, lastlines_dirpath)\n",
      "   while pipes:\n",
      "      (lines, bad_pipes) = poll_tail_pipes(pipes, lastlines_dirpath, waitsecs)\n",
      "      for bad in bad_pipes:\n",
      "         pipes.pop(bad)\n",
      "      try:\n",
      "         outstream.writelines((['\\n'] + lines))\n",
      "         outstream.flush()\n",
      "      except (IOError, OSError) as e:\n",
      "         break\n",
      "   snuff(procs.values())\n",
      "------------------------ declaration ----------------------------------\n",
      "def follow_files(follow_paths, outstream, lastlines_dirpath=None, waitsecs=5):\n",
      " 'Launch tail on a set of files and merge their output into outstream.\n",
      " Args:\n",
      " follow_paths: list; Local paths to launch tail on.\n",
      " outstream: file; Output stream to write aggregated lines to.\n",
      " lastlines_dirpath: Local dirpath to record last lines seen in.\n",
      " waitsecs: int; Timeout for poll_tail_pipes.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if (not isinstance(data, list)):\n",
      "      raise errors.AnsibleFilterError('|failed   expects   to   filter   on   a   list')\n",
      "   if (not isinstance(keys, list)):\n",
      "      raise errors.AnsibleFilterError('|failed   expects   first   param   is   a   list')\n",
      "   retval = [oo_select_keys(item, keys) for item in data]\n",
      "   return oo_flatten(retval)\n",
      "------------------------ declaration ----------------------------------\n",
      "def oo_select_keys_from_list(data, keys):\n",
      " 'This returns a list, which contains the value portions for the keys\n",
      " Ex: data = { \\'a\\':1, \\'b\\':2, \\'c\\':3 }\n",
      " keys = [\\'a\\', \\'c\\']\n",
      " returns [1, 3]'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   user = urllib.unquote(user)\n",
      "   sys.stderr.write((((('checking   permissions   for   user   ' + user) + '   on   api   ') + api_name) + '\\n'))\n",
      "   secret_file = open('/etc/appscale/secret.key', 'r')\n",
      "   secret = secret_file.read()\n",
      "   secret = secret[0:(-1)]\n",
      "   secret_file.close()\n",
      "   uaserver_file = open('/etc/appscale/hypersoap', 'r')\n",
      "   uaserver = uaserver_file.read()\n",
      "   uaserver_file.close()\n",
      "   server = SOAPpy.SOAPProxy((('https://' + uaserver) + ':4343'))\n",
      "   capabilities = server.get_capabilities(user, secret)\n",
      "   if (not isinstance(capabilities, str)):\n",
      "      return False\n",
      "   capabilities = capabilities.split(':')\n",
      "   sys.stderr.write((((('user   ' + user) + '   has   the   following   capabilities:   ') + str(capabilities)) + '\\n'))\n",
      "   if (api_name in capabilities):\n",
      "      return True\n",
      "   else:\n",
      "      return False\n",
      "------------------------ declaration ----------------------------------\n",
      "def is_user_capable(user, api_name):\n",
      " 'Checks to see if the given user has access to user a particular API.\n",
      " Args:\n",
      " user: The current user email\n",
      " api_name: The API we\\'re checking to see if the user has permission\n",
      " Returns:\n",
      " True is capable, False otherwise'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   if any([getattr(c.op, 'check_input', config.check_input) for (c, _) in r.clients if (not isinstance(c, string_types))]):\n",
      "      if any([getattr(c.op, 'check_broadcast', True) for (c, _) in r.clients if (not isinstance(c, string_types))]):\n",
      "         c_extract = r.type.c_extract(name, sub, True)\n",
      "      else:\n",
      "         try:\n",
      "            c_extract = r.type.c_extract(name, sub, True, check_broadcast=False)\n",
      "         except TypeError as e:\n",
      "            c_extract = r.type.c_extract(name, sub, True)\n",
      "   else:\n",
      "      c_extract = r.type.c_extract(name, sub, False)\n",
      "   pre = ('\\n            py_%(name)s   =   PyList_GET_ITEM(storage_%(name)s,   0);\\n            {Py_XINCREF(py_%(name)s);}\\n            ' % locals())\n",
      "   return (pre + c_extract)\n",
      "------------------------ declaration ----------------------------------\n",
      "def get_c_extract(r, name, sub):\n",
      " 'Wrapper around c_extract that initializes py_name from storage.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   return '   '.join(map(_stringify, value))\n",
      "------------------------ declaration ----------------------------------\n",
      "def _join(value):\n",
      " 'Internal function.'\n",
      "=========================================================================\n",
      "------------------------ body ----------------------------------\n",
      "   ctx = zmq.Context.instance()\n",
      "   s = ctx.socket(zmq.PUSH)\n",
      "   s.connect(url)\n",
      "   print(('Producing   %s' % ident))\n",
      "   for i in range(MSGS):\n",
      "      s.send((u'%s:   %i' % (ident, time.time())).encode('utf8'))\n",
      "      time.sleep(1)\n",
      "   print(('Producer   %s   done' % ident))\n",
      "   s.close()\n",
      "------------------------ declaration ----------------------------------\n",
      "def produce(url, ident):\n",
      " 'Produce messages'\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(100):\n",
    "    print('------------------------ body ----------------------------------')\n",
    "    print(transform(bodies1[idx]))\n",
    "    print('------------------------ declaration ----------------------------------')\n",
    "    print(transform(bodies2[idx]))\n",
    "    print('=========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
